# -*- coding: utf-8 -*-
"""Ass_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ibEqCs4xV1gJVqzxit4dzcyGihxSFP1B
"""

import pandas as pd
df=pd.read_csv("Heart.csv")

print(df.to_string())

print(df.std())  #Standard Deviation

print(df.var())  #Variance

print(df.cov())   #Covariance

print(df.corr())  #Corelation

bins = pd.cut(df['Age'], bins=5, labels=False)
df['Age'] = bins

print(df.head(5))

features = df.drop(columns=['AHD'])
# Get the number of independent features
independent_features = features.shape[1]
# .shape : returns a tuple representing its dimensions (number of rows, number of columns)
# features.shape[1] : retrieves the number of columns, which corresponds to the number of independent features.
print(independent_features)

from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
# Select attributes to normalize
attributes_to_normalize = ['RestBP', 'Chol', 'MaxHR']

# Min-Max normalization
min_max_scaler = MinMaxScaler()
ans=df[attributes_to_normalize] = min_max_scaler.fit_transform(df[attributes_to_normalize])
print(ans)

# Z-score normalization
z_score_scaler = StandardScaler()
ans=df[attributes_to_normalize] = z_score_scaler.fit_transform(df[attributes_to_normalize])
print(ans)

# Decimal scaling normalization
decimal_scaler = RobustScaler()
ans=df[attributes_to_normalize] = decimal_scaler.fit_transform(df[attributes_to_normalize])
print(ans)